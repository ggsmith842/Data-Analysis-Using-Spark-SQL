{"cells":[{"cell_type":"markdown","source":["# Part II: Data Prep\n\nIn this notebook we will look at market data using the yfinance python package."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"91250ede-78b1-4772-8073-4241a60f2f60","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql import SQLContext\nfrom pyspark import SparkContext\nfrom pyspark import Row\n\n# install yfinance package to get data from yahoo finance\n#!pip install yfinance\nimport yfinance as yf\nimport pandas as pd"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"adba7faa-535b-4eea-a743-7d944358e8b3","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# build spark session\nspark = SparkSession.builder.appName(\"Analyze Market Data\").getOrCreate()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"81727418-6bcf-4a5f-a1d7-41bf085602a0","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Let's get some data!\n\nUsing yfinance we will get some data for a few different assets. \n\n*Note: we are defining an asset as an investment asset (ie a stock, bond, or etf)*"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"b5abd563-9f5c-413a-8d3d-41b2e9008a3f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# define assets in portfolio\nassetList = [\"ORCL\", \"BLK\", \"UNH\"]\n\n# get asset overview data\nasset_data = [yf.Ticker(asset).info for asset in assetList]\n\n# get price data for assets\nprice_data = pd.DataFrame()\nfor asset in assetList:\n    data = yf.download(asset, start=\"2017-01-01\", end=\"2023-04-23\")\n    data[\"Symbol\"] = asset\n    price_data = pd.concat([price_data, data])\n\nprice_data.reset_index(inplace=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"c5a53c48-3b1c-4e07-9af6-cc5b8e1f9cbd","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["\r[*********************100%***********************]  1 of 1 completed\n\r[*********************100%***********************]  1 of 1 completed\n\r[*********************100%***********************]  1 of 1 completed\n"]}],"execution_count":0},{"cell_type":"code","source":["# create spark dataframes\nasset_df = spark.createDataFrame(asset_data)\nprice_df = spark.createDataFrame(price_data)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"bf69ab70-09a6-48c7-af04-ab61124fc469","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# let's look at the first 5 columns in our asset overview dataframe (this data has alot of columns)\nasset_df.columns\n\n# we only care about a few of these so let's drop the ones we don't care about\nasset_info_df = asset_df.select(\n    \"symbol\",\n    \"beta\",\n    \"totalRevenue\",\n    \"totalDebt\",\n    \"overallRisk\",\n    \"auditRisk\",\n    \"exDividendDate\",\n    \"dividendYield\",\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"0d6c513b-0128-46e5-911b-c7b945c1e7a5","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["asset_info_df.show()\nprint(asset_info_df.schema)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"9c9f0eeb-dfdb-4404-9095-77d1fc6c3736","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+------+--------+------------+-----------+-----------+---------+--------------+-------------+\n|symbol|    beta|totalRevenue|  totalDebt|overallRisk|auditRisk|exDividendDate|dividendYield|\n+------+--------+------------+-----------+-----------+---------+--------------+-------------+\n|  ORCL|0.995847| 47957999616|91810996224|         10|        8|    1681084800|       0.0168|\n|   BLK|1.289273| 17417000960| 8488999936|          3|        4|    1678060800|       0.0297|\n|   UNH| 0.67993|335943991296|70587998208|          3|       10|    1678406400|       0.0136|\n+------+--------+------------+-----------+-----------+---------+--------------+-------------+\n\nStructType([StructField('symbol', StringType(), True), StructField('beta', DoubleType(), True), StructField('totalRevenue', LongType(), True), StructField('totalDebt', LongType(), True), StructField('overallRisk', LongType(), True), StructField('auditRisk', LongType(), True), StructField('exDividendDate', LongType(), True), StructField('dividendYield', DoubleType(), True)])\n"]}],"execution_count":0},{"cell_type":"code","source":["price_df.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"62d9542e-91b3-4ee2-89e7-4de06ddc7e90","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------------+------------------+------------------+------------------+------------------+------------------+--------+------+\n|               Date|              Open|              High|               Low|             Close|         Adj Close|  Volume|Symbol|\n+-------------------+------------------+------------------+------------------+------------------+------------------+--------+------+\n|2017-01-03 00:00:00| 38.45000076293945|38.689998626708984| 38.29999923706055| 38.54999923706055|34.786827087402344|11051300|  ORCL|\n|2017-01-04 00:00:00| 38.54999923706055| 38.91999816894531| 38.54999923706055|  38.7400016784668| 34.95830154418945| 9545500|  ORCL|\n|2017-01-05 00:00:00| 38.66999816894531| 38.95000076293945| 38.40999984741211| 38.63999938964844| 34.86805725097656|12064700|  ORCL|\n|2017-01-06 00:00:00|             38.75|             38.75|38.380001068115234| 38.45000076293945|34.696598052978516|14829700|  ORCL|\n|2017-01-09 00:00:00|38.529998779296875| 39.45000076293945|38.470001220703125|39.029998779296875| 35.21997833251953|15587900|  ORCL|\n+-------------------+------------------+------------------+------------------+------------------+------------------+--------+------+\nonly showing top 5 rows\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["We can also filter our data. Let's say we only want the price data for the current month."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"014bc7ec-2f73-4b32-9245-f5011445b1aa","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["price_df.filter((price_df.Date >= \"2023-04-01\") & (price_df.Date < \"2023-05-01\")).show(\n    5\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"0dcfe4f0-1d6b-445b-908d-ee4e9eaeae52","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------------+-----------------+-----------------+-----------------+-----------------+-----------------+-------+------+\n|               Date|             Open|             High|              Low|            Close|        Adj Close| Volume|Symbol|\n+-------------------+-----------------+-----------------+-----------------+-----------------+-----------------+-------+------+\n|2023-04-03 00:00:00|92.37999725341797|             94.0|92.08999633789062|93.91999816894531| 93.5283432006836|8410900|  ORCL|\n|2023-04-04 00:00:00| 93.8499984741211| 94.0199966430664|92.93000030517578|             94.0| 93.6080093383789|6651500|  ORCL|\n|2023-04-05 00:00:00|93.62000274658203|95.11000061035156| 93.4800033569336|94.88999938964844|94.49429321289062|7478800|  ORCL|\n|2023-04-06 00:00:00|94.33000183105469|96.08000183105469|93.98999786376953|95.91999816894531| 95.5199966430664|9146200|  ORCL|\n|2023-04-10 00:00:00|94.68000030517578|95.11000061035156|93.55000305175781|93.76000213623047|93.76000213623047|8681500|  ORCL|\n+-------------------+-----------------+-----------------+-----------------+-----------------+-----------------+-------+------+\nonly showing top 5 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# now let's create a temp view\nprice_df.createOrReplaceTempView(\"prices\")\n\n# now we can execute our SQL query\nprices = spark.sql(\"SELECT * FROM prices\")\nprices.columns"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"eeb38632-a43b-48e7-900b-8f06056f005f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[10]: ['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'Symbol']"]}],"execution_count":0},{"cell_type":"markdown","source":["Now let's create a view of our asset details table"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"8f143b89-e12b-4268-a411-66408f900ff8","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["asset_info_df.createOrReplaceTempView(\"details\")\n\n# now we can execute our SQL query\ndetails = spark.sql(\"SELECT * FROM details\")\ndetails.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"5b42b7cd-6184-447c-837c-8b237e7c3c5d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+------+--------+------------+-----------+-----------+---------+--------------+-------------+\n|symbol|    beta|totalRevenue|  totalDebt|overallRisk|auditRisk|exDividendDate|dividendYield|\n+------+--------+------------+-----------+-----------+---------+--------------+-------------+\n|  ORCL|0.995847| 47957999616|91810996224|         10|        8|    1681084800|       0.0168|\n|   BLK|1.289273| 17417000960| 8488999936|          3|        4|    1678060800|       0.0297|\n|   UNH| 0.67993|335943991296|70587998208|          3|       10|    1678406400|       0.0136|\n+------+--------+------------+-----------+-----------+---------+--------------+-------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["print(\n    f\"Total rows in each view: prices = {prices.count()}, details = { details.count()}\"\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"23f44466-e68e-4c3d-a836-57fbdc77b927","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Total rows in each view: prices = 4758, details = 3\n"]}],"execution_count":0},{"cell_type":"markdown","source":["## Write Data to Delta Tables\n\nBecause notebooks tend to become unreadable as you do more and more work, we want to write our prepared data to a central location. This way we can access it from any other notebook or script without having to run all the code in this notebook first. \n\nThis makes our projects more maneagable as we don't have one monolithic notebook that has to be run everytime. You should think about how the actions the notebook performs are tied to different roles. This notebook for example may never be used by a data analyst or scientist, but a data engineer would find the steps here very useful.\n\nFirst we need to update a column name in our prices dataframe. Spark doesn't like spaces in column names. You can set a property that allows this, but for simplicity we will just rename it. Renaming columns is also a very common task in data preparation so its good practice."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dc0b784e-6736-4ca0-b4e8-87e4856e75ff","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#rename Adjusted Closing price column to remove the space\nprice_df = price_df.withColumnRenamed('Adj Close', 'adjClose')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"6a3df959-0c9a-4864-8e07-f0423ee89e6d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#write our spark dataframes to delta tables\nasset_info_df.write.saveAsTable(\"details\")\nprice_df.write.saveAsTable(\"prices\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"bd875821-d455-4e92-8123-197d9f5a6a34","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Great! Now we are ready to start executing queries on the data from any notebook in our spark environment."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"44a3405f-cd81-4748-9683-49af4b7f4ba2","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Appendix\n\n#### What are Delta Tables\n\nDelta Tables are Databricks default method for storing data in your Databricks enviroment. There is more information on them in the documentation which you can find using the link below. \n\nref: [docs.databricks/delta](https://docs.databricks.com/delta/index.html)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"1c267ce2-1453-4253-88d1-89ee02c748c9","inputWidgets":{},"title":""}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Part II: Data Preparation for Spark","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1831395769957316}},"nbformat":4,"nbformat_minor":0}
